{
  "master": {
    "tasks": [
      {
        "id": "176",
        "title": "Create data/conversations directory and conversation_store.py",
        "description": "Implement conversation history storage using JSON files in data/conversations/{user_id}.json with the specified structure including timestamps and metadata.",
        "details": "Create directory data/conversations if not exists. Implement ConversationStore class with methods: load_conversation(user_id) -> dict or empty dict, save_conversation(user_id, data: dict), add_message(user_id, role: str, content: str). Use datetime.utcnow().isoformat() for timestamps. Ensure atomic writes using file locks or temp files. Structure exactly as PRD: {'user_id': str, 'created_at': str, 'updated_at': str, 'messages': list[dict]}. Handle file not found gracefully by creating new conversation.",
        "testStrategy": "Unit tests: test_load_nonexistent(), test_save_load_roundtrip(), test_add_message_appends_correctly(), test_timestamps_updated(). Integration test: multiple add_message calls persist across loads. Test with 100 concurrent writes to verify thread-safety.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-26T08:27:08.220Z"
      },
      {
        "id": "177",
        "title": "Implement FastAPI server with /chat endpoint and /health",
        "description": "Create scripts/testing/api.py with FastAPI app, POST /chat endpoint matching exact request/response schema, GET /health, configurable port from env, .env loading.",
        "details": "pip install fastapi uvicorn python-dotenv. from fastapi import FastAPI, HTTPException. app = FastAPI(). Load env: from dotenv import load_dotenv; load_dotenv(). @app.get('/health') -> {'status': 'ok'}. @app.post('/chat') with Pydantic models: class ChatRequest(BaseModel): user_id: str, message: str, platform: Optional[str]='instagram'. Response: class ChatResponse(BaseModel): response: str, conversation_ended: bool. Port: uvicorn.run('api:app', port=int(os.getenv('PORT', 8000))). Error handling: HTTPException(400) for missing fields.",
        "testStrategy": "Run server, curl GET /health expect 200. curl POST /chat with valid payload expect 200 with response. Test invalid JSON (400), missing user_id (400), empty message (400). Verify server restarts cleanly.",
        "priority": "high",
        "dependencies": [
          "176"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-26T08:30:15.643Z"
      },
      {
        "id": "178",
        "title": "Integrate conversation loading/saving into /chat endpoint",
        "description": "Load conversation history at start of /chat, save updated history after response generation.",
        "details": "In POST /chat: 1. conv = conversation_store.load_conversation(user_id). 2. conv['messages'].append({'role': 'fan', 'content': message, 'timestamp': now}). 3. Update conv['updated_at'] = now. 4. After generating response, append {'role': 'her', 'content': response, 'timestamp': now}. 5. conversation_store.save_conversation(user_id, conv). Ensure created_at set only on first save.",
        "testStrategy": "Send first message to new user_id, verify file created with correct structure. Send second message same user_id, verify history loaded and both messages present. Delete file between requests to test fresh creation.",
        "priority": "high",
        "dependencies": [
          "176",
          "177"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-26T08:30:15.645Z"
      },
      {
        "id": "179",
        "title": "Integrate existing LLM modules into API workflow",
        "description": "Call ig_simple_prompt.py, llm_client.py, response_generator.py with conversation history to generate responses.",
        "details": "Import existing modules: from ig_simple_prompt import get_system_prompt; from llm_client import LLMClient; from response_generator import generate_response. In /chat after loading history: system_prompt = get_system_prompt(platform). llm = LLMClient(). full_prompt = system_prompt + format_messages(conv['messages']). response_data = generate_response(llm, full_prompt). response = response_data['response']. conversation_ended = response_data.get('conversation_ended', False). Handle any exceptions from existing modules gracefully.",
        "testStrategy": "Mock existing modules to return known responses. Test full flow: message in -> existing modules called with correct params -> expected response out. Verify conversation_ended flag set correctly for subscription messages.",
        "priority": "high",
        "dependencies": [
          "178"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-26T08:30:15.647Z"
      },
      {
        "id": "180",
        "title": "Implement conversation end detection using existing logic",
        "description": "Integrate conversation end detection from ig_auto_tester.py into response generation flow.",
        "details": "Import detection logic from ig_auto_tester.py (analyze how it detects subscription/refusal). After LLM response generation but before returning: ended = detect_conversation_end(message, response, conv_history). If ended, set conversation_ended: true and optionally append final message. Use heuristics: keywords like 'subbed', 'subscribed', 'onlyfans', 'of', 'refuse', 'no thanks'. Ensure doesn't interfere with normal conversation flow.",
        "testStrategy": "Test cases: 'just subbed!' -> ended=true, 'you in miami?' -> ended=false, 'no thanks bye' -> ended=true. Verify response still generated even when ended=true. Test edge cases: partial keywords, multiple messages.",
        "priority": "medium",
        "dependencies": [
          "179"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-26T08:30:15.649Z"
      },
      {
        "id": "181",
        "title": "Add comprehensive error handling and server hardening",
        "description": "Implement graceful error handling for all failure modes: bad requests, LLM failures, file I/O errors, existing module exceptions.",
        "details": "Wrap entire /chat logic in try/except. Specific handlers: ValidationError -> 422, LLMClient failures -> 503 with retry info, File I/O errors -> 500 log but don't crash, Existing module exceptions -> 500 generic error. Add request logging. Implement max conversation length (truncate old messages if >50). Add CORS if needed for phone automation. Graceful shutdown on SIGTERM.",
        "testStrategy": "Chaos testing: kill llm_client.py dependencies, corrupt JSON files, send malformed requests, network failures to Claude API. Verify server stays up, returns appropriate HTTP codes, logs errors, doesn't lose data. Load test: 100 concurrent requests to same user_id.",
        "priority": "medium",
        "dependencies": [
          "180"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-26T08:30:15.652Z"
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2025-12-26T08:30:15.652Z",
      "taskCount": 6,
      "completedCount": 6,
      "tags": [
        "master"
      ]
    }
  }
}